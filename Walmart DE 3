### Required Experience and Technical Skills:

- **3+ years of Data Engineering experience** in **Big Data technologies** such as **Hadoop**, **Spark**, and **distributed computing frameworks**.
- **Hands-on experience in Scala** for building robust data solutions.
- Strong proficiency in **SQL** with expertise in **query optimization** for performance improvement.
- Solid understanding of **data modeling**, including conceptual, physical, and logical models.
- Proficient in **data design** and **data transformation** techniques for creating efficient data pipelines.
- Expertise in **data governance**, ensuring compliance with **business policies** and **regulatory standards**.
- Practical experience with **cloud technologies**, specifically **Google Cloud Platform (GCP)**.
- Exposure to **streaming data** and technologies like **Kafka** or **structured streaming**.
- Experience with **CI/CD tools** for automating data workflows and deployment.
- Proficient in using **orchestration and scheduling tools** like **Airflow** or **Automic** for workflow management.
- Knowledge in **data processing** and **data warehousing concepts**, including **Slowly Changing Dimensions (SCD types)**.
